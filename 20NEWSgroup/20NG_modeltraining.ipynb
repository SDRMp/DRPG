{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dstype = '20NG' \n",
    "mname = 'debertaV3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/bhairavi/om/om5/20NG/debertaV3_20NG\n"
     ]
    }
   ],
   "source": [
    " \n",
    "modelpath = 'microsoft/deberta-v3-base'\n",
    "# modelpath = \"bert-base-uncased\"\n",
    "\n",
    "\n",
    "datapath = None\n",
    "saveDIR = f\"/home/bhairavi/om/om5/{dstype}/{mname}_{dstype}\"\n",
    "print(saveDIR)\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# %%\n",
    "# %%\n",
    " \n",
    "import os\n",
    "import torch  \n",
    "\n",
    "torch.cuda.empty_cache() \n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict, load_metric\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "\n",
    "# %%\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('rungalileo/20_Newsgroups_Fixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'text', 'label'],\n",
       "        num_rows: 11314\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'text', 'label'],\n",
       "        num_rows: 7532\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(dataset['test'])\n",
    "train_df = pd.DataFrame(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['split'] = 'train'\n",
    "\n",
    "test_df['split'] = 'test'\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train_df, test_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18846, 4)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " \n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17683, 4)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'text', 'label', 'split'], dtype='object')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.',\n",
       " 'rec.autos')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'][0] , df['label'][0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1511</th>\n",
       "      <td>1511</td>\n",
       "      <td>...His account that is.\\n\\nMany important issu...</td>\n",
       "      <td>talk.politics.misc</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7196</th>\n",
       "      <td>7196</td>\n",
       "      <td>Well, after a lot of trawling through archives...</td>\n",
       "      <td>sci.crypt</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2616</th>\n",
       "      <td>2616</td>\n",
       "      <td>: \\n: Where could I find a description of the ...</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13378</th>\n",
       "      <td>2064</td>\n",
       "      <td>\\nWell, -I've- been reading t.p.m. for a while...</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11829</th>\n",
       "      <td>515</td>\n",
       "      <td>The Microsoft Windows Device Driver Kit (DDK) ...</td>\n",
       "      <td>comp.os.ms-windows.misc</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                               text  \\\n",
       "1511   1511  ...His account that is.\\n\\nMany important issu...   \n",
       "7196   7196  Well, after a lot of trawling through archives...   \n",
       "2616   2616  : \\n: Where could I find a description of the ...   \n",
       "13378  2064  \\nWell, -I've- been reading t.p.m. for a while...   \n",
       "11829   515  The Microsoft Windows Device Driver Kit (DDK) ...   \n",
       "\n",
       "                         label  split  \n",
       "1511        talk.politics.misc  train  \n",
       "7196                 sci.crypt  train  \n",
       "2616             comp.graphics  train  \n",
       "13378    talk.politics.mideast   test  \n",
       "11829  comp.os.ms-windows.misc   test  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# %%\n",
    "\n",
    "# %%\n",
    "df.sample(5)\n",
    "\n",
    "\n",
    "# %%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"id\"], inplace=True)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I was wondering if anyone out there could enli...</td>\n",
       "      <td>rec.autos</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A fair number of brave souls who upgraded thei...</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>well folks, my mac plus finally gave up the gh...</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text                  label  \\\n",
       "0  I was wondering if anyone out there could enli...              rec.autos   \n",
       "1  A fair number of brave souls who upgraded thei...  comp.sys.mac.hardware   \n",
       "2  well folks, my mac plus finally gave up the gh...  comp.sys.mac.hardware   \n",
       "\n",
       "   split  \n",
       "0  train  \n",
       "1  train  \n",
       "2  train  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "# %%\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# %%\n",
    "le = LabelEncoder()\n",
    "df['target'] = le.fit_transform(df['label'])\n",
    "\n",
    " \n",
    "\n",
    "# %%\n",
    "\n",
    "# %%\n",
    "numlabel = df['target'].nunique()\n",
    "numlabel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'].nunique(), df['label'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'label', 'split', 'target'], dtype='object')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numlabel = df['target'].nunique()\n",
    "numlabel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(lambda x: str(x)[:512] if isinstance(x, float) else x[:512])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhairavi/om/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/home/bhairavi/om/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DebertaV2ForSequenceClassification(\n",
       "  (deberta): DebertaV2Model(\n",
       "    (embeddings): DebertaV2Embeddings(\n",
       "      (word_embeddings): Embedding(128100, 768, padding_idx=0)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): StableDropout()\n",
       "    )\n",
       "    (encoder): DebertaV2Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (rel_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (pooler): ContextPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): StableDropout()\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=20, bias=True)\n",
       "  (dropout): StableDropout()\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "device = \"cuda\"  # the device to load the model onto\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelpath)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(modelpath, num_labels=numlabel)\n",
    "\n",
    "# Move the model to the specified device\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['token_length'] = df['text'].apply(lambda x: len(tokenizer.tokenize(x)))\n",
    "\n",
    "# # Calculate the maximum token length\n",
    "# max_length = df['token_length'].max()\n",
    "\n",
    "# # Calculate the next maximum token length\n",
    "# next_max_token_length = df['token_length'].nlargest(2).iloc[1]\n",
    "\n",
    "# # Calculate the average token length\n",
    "# average_token_length = df['token_length'].mean()\n",
    "\n",
    "# # Display the results\n",
    "# print(f\"Maximum token length: {max_length}\")\n",
    "# print(f\"Next maximum token length: {next_max_token_length}\")\n",
    "# print(f\"Average token length: {average_token_length:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df[df['split'] == 'train'].drop(columns=['split'])\n",
    "\n",
    "test_df = df[df['split'] == 'test'].drop(columns=['split'])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10664, 3), (7019, 3))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_df = train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9064/9064 [00:03<00:00, 2414.39 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:00<00:00, 2518.03 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7019/7019 [00:02<00:00, 2524.41 examples/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %%\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Splitting off the test set with 5% of the data\n",
    "# sss = StratifiedShuffleSplit(n_splits=1, test_size=0.05, random_state=42)  # 5% for test\n",
    "# for train_val_idx, test_idx in sss.split(df, df['target']):\n",
    "#     train_val_df = df.iloc[train_val_idx]\n",
    "#     test_df = df.iloc[test_idx]\n",
    "\n",
    "# Further split train_val_df into train and validation sets with validation set being 15.79% of the remaining data\n",
    "# (which is equivalent to 15% of the original dataset size)\n",
    "sss_val = StratifiedShuffleSplit(n_splits=1, test_size=0.15, random_state=42)  # ~15.79% of remaining data\n",
    "for train_idx, val_idx in sss_val.split(train_val_df, train_val_df['target']):\n",
    "    train_df = train_val_df.iloc[train_idx]\n",
    "    val_df = train_val_df.iloc[val_idx]\n",
    " \n",
    "\n",
    "def tokenize_and_format(examples):\n",
    "    # Tokenize the texts\n",
    "    tokenized_inputs = tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=max_length)\n",
    "    tokenized_inputs['label'] = list(map(int, examples['target']))\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Convert pandas DataFrame to Hugging Face's Dataset\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "eval_dataset = Dataset.from_pandas(val_df) \n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Map the tokenization function across the datasets\n",
    "train_dataset = train_dataset.map(tokenize_and_format, batched=True,batch_size=16)\n",
    "eval_dataset = eval_dataset.map(tokenize_and_format, batched=True,batch_size=16) \n",
    "test_dataset = test_dataset.map(tokenize_and_format, batched=True,batch_size=16)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhairavi/om/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3399' max='3399' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3399/3399 1:02:11, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.855800</td>\n",
       "      <td>1.081792</td>\n",
       "      <td>0.643025</td>\n",
       "      <td>0.636911</td>\n",
       "      <td>0.668750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.026000</td>\n",
       "      <td>0.938876</td>\n",
       "      <td>0.725880</td>\n",
       "      <td>0.733638</td>\n",
       "      <td>0.731875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.575200</td>\n",
       "      <td>0.934383</td>\n",
       "      <td>0.739221</td>\n",
       "      <td>0.732590</td>\n",
       "      <td>0.749375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhairavi/om/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/bhairavi/om/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3399, training_loss=1.059129979898734, metrics={'train_runtime': 3733.5951, 'train_samples_per_second': 7.283, 'train_steps_per_second': 0.91, 'total_flos': 7155800400494592.0, 'train_loss': 1.059129979898734, 'epoch': 3.0})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# %%\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average='weighted')\n",
    "    precision = precision_score(labels, preds, average='weighted')\n",
    "    recall = recall_score(labels, preds, average='weighted')\n",
    "    return {\n",
    "        'eval_f1': f1,\n",
    "        'eval_precision': precision,\n",
    "        'eval_recall': recall,\n",
    "    }\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "# %%\n",
    " \n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",  # Use 'epoch' to evaluate at the end of each epoch\n",
    "    save_strategy=\"epoch\",  # Also use 'epoch' to save at the end of each epoch\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,  # Load the best model at the end of training based on metric\n",
    "    metric_for_best_model='f1',  # Define the metric for evaluating the best model\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    report_to=[] \n",
    ")\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args ,  # Here you will need to make sure that the Trainer is set up correctly\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    " \n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/bhairavi/om/om5/20NG/debertaV3_20NG'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saveDIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/bhairavi/om/om5/20NG/debertaV3_20NG/tokenizer_config.json',\n",
       " '/home/bhairavi/om/om5/20NG/debertaV3_20NG/special_tokens_map.json',\n",
       " '/home/bhairavi/om/om5/20NG/debertaV3_20NG/spm.model',\n",
       " '/home/bhairavi/om/om5/20NG/debertaV3_20NG/added_tokens.json',\n",
       " '/home/bhairavi/om/om5/20NG/debertaV3_20NG/tokenizer.json')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_directory = saveDIR\n",
    " \n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(save_directory)\n",
    "\n",
    "# Save the tokenizer (optional, but recommended)\n",
    "tokenizer.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhairavi/om/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/bhairavi/om/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m keywords class evaluation detection RESULTS\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "               rec.autos     0.5238    0.6567    0.5828        67\n",
      "   comp.sys.mac.hardware     0.6456    0.6145    0.6296        83\n",
      "           comp.graphics     0.6706    0.6867    0.6786        83\n",
      "               sci.space     0.5644    0.6786    0.6162        84\n",
      "      talk.politics.guns     0.7143    0.6790    0.6962        81\n",
      "                 sci.med     0.7895    0.8721    0.8287        86\n",
      "comp.sys.ibm.pc.hardware     0.8452    0.8353    0.8402        85\n",
      " comp.os.ms-windows.misc     0.8077    0.7778    0.7925        81\n",
      "         rec.motorcycles     0.7159    0.7590    0.7368        83\n",
      "      talk.religion.misc     0.9583    0.8415    0.8961        82\n",
      "            misc.forsale     0.9080    0.9405    0.9240        84\n",
      "             alt.atheism     0.8451    0.7059    0.7692        85\n",
      "         sci.electronics     0.7229    0.7143    0.7186        84\n",
      "          comp.windows.x     0.8851    0.8953    0.8902        86\n",
      "        rec.sport.hockey     0.8111    0.8588    0.8343        85\n",
      "      rec.sport.baseball     0.6442    0.7701    0.7016        87\n",
      "  soc.religion.christian     0.7711    0.8101    0.7901        79\n",
      "   talk.politics.mideast     0.8625    0.8846    0.8734        78\n",
      "      talk.politics.misc     0.6250    0.6818    0.6522        66\n",
      "               sci.crypt     0.0000    0.0000    0.0000        51\n",
      "\n",
      "                accuracy                         0.7494      1600\n",
      "               macro avg     0.7155    0.7331    0.7226      1600\n",
      "            weighted avg     0.7326    0.7494    0.7392      1600\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhairavi/om/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/bhairavi/om/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/bhairavi/om/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %%\n",
    "from colorama import Fore, Style\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# eval dataset performance so that keywords_classes can be fixed\n",
    "\n",
    "# %%\n",
    "results = trainer.evaluate()\n",
    "\n",
    "# Predict using the trained model to get labels and predictions\n",
    "predictions, labels, _ = trainer.predict(eval_dataset)\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "\n",
    "# %%\n",
    "from sklearn.metrics import classification_report\n",
    "# Generate the classification report\n",
    "report = classification_report(\n",
    "    labels,\n",
    "    predictions,\n",
    "    target_names=df['label'].unique() , # Adjust this line as per your dataset\n",
    "    digits=4\n",
    ")\n",
    "print(Fore.CYAN,\"keywords class evaluation detection RESULTS\")\n",
    "print(report)\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# skyline\n",
    "\n",
    "# %%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mTEST DATA IS OUR SKYLINE RESULT\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bhairavi/om/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "               rec.autos     0.4045    0.5621    0.4704       290\n",
      "   comp.sys.mac.hardware     0.6927    0.6703    0.6813       370\n",
      "           comp.graphics     0.6555    0.6911    0.6728       369\n",
      "               sci.space     0.5825    0.6604    0.6190       374\n",
      "      talk.politics.guns     0.7035    0.6177    0.6578       361\n",
      "                 sci.med     0.8090    0.8243    0.8166       370\n",
      "comp.sys.ibm.pc.hardware     0.8472    0.8886    0.8674       368\n",
      " comp.os.ms-windows.misc     0.8348    0.7853    0.8093       354\n",
      "         rec.motorcycles     0.8000    0.7424    0.7701       361\n",
      "      talk.religion.misc     0.9215    0.8661    0.8930       366\n",
      "            misc.forsale     0.9119    0.9312    0.9215       378\n",
      "             alt.atheism     0.8123    0.7333    0.7708       360\n",
      "         sci.electronics     0.6446    0.6621    0.6532       367\n",
      "          comp.windows.x     0.8861    0.8598    0.8728       371\n",
      "        rec.sport.hockey     0.7608    0.7690    0.7649       368\n",
      "      rec.sport.baseball     0.6613    0.8684    0.7509       380\n",
      "  soc.religion.christian     0.5903    0.6499    0.6186       337\n",
      "   talk.politics.mideast     0.8290    0.8102    0.8195       353\n",
      "      talk.politics.misc     0.4885    0.5103    0.4992       292\n",
      "               sci.crypt     0.4615    0.0261    0.0494       230\n",
      "\n",
      "                accuracy                         0.7240      7019\n",
      "               macro avg     0.7149    0.7064    0.6989      7019\n",
      "            weighted avg     0.7258    0.7240    0.7165      7019\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# %%\n",
    "print(Fore.RED +\"TEST DATA IS OUR SKYLINE RESULT\")\n",
    " \n",
    "results = trainer.evaluate()\n",
    "\n",
    "# Predict using the trained model to get labels and predictions\n",
    "predictions, labels, _ = trainer.predict(test_dataset)\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "\n",
    "# %%\n",
    "from sklearn.metrics import classification_report\n",
    "# Generate the classification report\n",
    "report = classification_report(\n",
    "    labels,\n",
    "    predictions,\n",
    "    target_names=df['label'].unique() , # Adjust this line as per your dataset\n",
    "    digits=4\n",
    ")\n",
    "\n",
    "print(report)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "om",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
